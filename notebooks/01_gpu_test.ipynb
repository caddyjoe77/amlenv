{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Test Notebook\n",
    "This notebook verifies that GPU acceleration is working in your Azure ML environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Current device: {torch.cuda.current_device()}')\n",
    "    print(f'Device name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple matrix multiplication test\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create large random matrices\n",
    "size = 10000\n",
    "a_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "b_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "# CPU test\n",
    "start = time.time()\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "# GPU test\n",
    "a_gpu = torch.tensor(a_cpu).cuda()\n",
    "b_gpu = torch.tensor(b_cpu).cuda()\n",
    "\n",
    "# Warm-up\n",
    "_ = torch.mm(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Actual timing\n",
    "start = time.time()\n",
    "c_gpu = torch.mm(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "gpu_time = time.time() - start\n",
    "\n",
    "print(f'CPU Time: {cpu_time:.4f} seconds')\n",
    "print(f'GPU Time: {gpu_time:.4f} seconds')\n",
    "print(f'Speedup: {cpu_time/gpu_time:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Upload this notebook to your Azure ML workspace\n",
    "2. Create a new compute instance in the workspace\n",
    "3. Run the cells to verify GPU acceleration\n",
    "4. Start developing your ML models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
